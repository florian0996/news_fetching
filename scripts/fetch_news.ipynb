{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f6198a-2b05-45fa-aaf3-3f24039516c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (5.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b616696a-eb87-487e-8541-aedca1a5e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yake in /opt/anaconda3/lib/python3.12/site-packages (0.4.8)\n",
      "Requirement already satisfied: tabulate in /opt/anaconda3/lib/python3.12/site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: click>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from yake) (8.1.7)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from yake) (1.26.4)\n",
      "Requirement already satisfied: segtok in /opt/anaconda3/lib/python3.12/site-packages (from yake) (1.5.11)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from yake) (3.2.1)\n",
      "Requirement already satisfied: jellyfish in /opt/anaconda3/lib/python3.12/site-packages (from yake) (1.0.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from segtok->yake) (2023.10.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a91d248-5325-4126-8cec-8498f3ab80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import feedparser\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b9fed7-4d43-466f-941a-0aff79fa9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"en\", n=1, top=10)\n",
    "\n",
    "def extract_keywords(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw for kw, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0009964-b498-4e29-bedd-59ed9fef537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_query(text, query):\n",
    "    query_terms = [term.strip().lower() for term in query.split(\"OR\")]\n",
    "    text_lower = text.lower()\n",
    "    return any(term in text_lower for term in query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad76967a-60bc-4ab7-b69b-d7104b688649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "NEWSAPI_KEY = \"186dd4ccd2234f6a89f850bf16effb06\"\n",
    "\n",
    "QUERY = (\n",
    "    \"fintech startup OR digital lending OR credit platform OR loan service OR Exaloan \"\n",
    "    \"OR peer-to-peer lending OR online loan platform OR investment platform \"\n",
    "    \"OR digital wealth management OR fractional investing OR seed funding OR fintech VC OR risk assessment\"\n",
    ")\n",
    "\n",
    "QUERY_short = (\n",
    "    \"credit OR loan OR Exaloan OR lending\"\n",
    ")\n",
    "\n",
    "LANGUAGE = \"en\"\n",
    "PAGE_SIZE = 100\n",
    "\n",
    "# Switch to enable/disable article filtering\n",
    "ENABLE_FILTERING = False  # Set to False to bypass QUERY-based filtering\n",
    "\n",
    "def apply_query_filter(articles):\n",
    "    \"\"\"\n",
    "    Filters a list of article dicts based on the global QUERY if ENABLE_FILTERING is True.\n",
    "    \"\"\"\n",
    "    # if the switch is off, skip all filtering\n",
    "    if not ENABLE_FILTERING:\n",
    "        return articles\n",
    "\n",
    "    # otherwise, only keep articles whose title+content match at least one OR-term\n",
    "    filtered = [\n",
    "        a for a in articles\n",
    "        if matches_query(\n",
    "            a.get(\"title\", \"\") + \" \" + a.get(\"content\", \"\"),\n",
    "            QUERY\n",
    "        )\n",
    "    ]\n",
    "    print(f\"→ {len(filtered)} articles after filtering.\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94d9bc1-41ae-4e9f-a6c1-a082cbc58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_FEEDS = {\n",
    "    \"Markets\":   \"https://feeds.bloomberg.com/markets/news.rss\",\n",
    "    \"Politics\":  \"https://feeds.bloomberg.com/politics/news.rss\",\n",
    "    \"Business\":  \"https://feeds.bloomberg.com/business/news.rss\",\n",
    "    \"Technology\":\"https://feeds.bloomberg.com/technology/news.rss\",\n",
    "    \"Economics\": \"https://feeds.bloomberg.com/economics/news.rss\",\n",
    "    \"Industries\":\"https://feeds.bloomberg.com/industries/news.rss\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d65f73-9a6e-49e2-be1f-e02a7aa1b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BLOOMBERG RSS FETCH ==========\n",
    "def fetch_bloomberg_rss():\n",
    "    print(\"Fetching Bloomberg RSS feeds...\")\n",
    "    all_articles = []\n",
    "    for name, feed_url in RSS_FEEDS.items():\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        for entry in feed.entries:\n",
    "            content = getattr(entry, 'summary', entry.get('description', ''))\n",
    "            all_articles.append({\n",
    "                \"source\":         f\"Bloomberg – {name} [RSS]\",\n",
    "                \"url\":            entry.link,\n",
    "                \"title\":          entry.title,\n",
    "                \"published_at\":   entry.published if \"published\" in entry else \"\",\n",
    "                \"content\":        content,\n",
    "                \"platforms_mentioned\": [],\n",
    "            })\n",
    "    print(f\"→ Bloomberg RSS: {len(all_articles)} articles fetched.\")\n",
    "\n",
    "    # ——— apply QUERY-based filtering if ENABLE_FILTERING is True ———\n",
    "    return apply_query_filter(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2db57e0-e91e-4cab-ad2b-ed9b4b937743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== NEWSAPI FETCH ==========\n",
    "def fetch_newsapi():\n",
    "    print(\"Fetching from NewsAPI...\")\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\":        \"lending OR credit\",\n",
    "        \"language\": LANGUAGE,\n",
    "        \"pageSize\": PAGE_SIZE,\n",
    "        \"sortBy\":   \"publishedAt\",\n",
    "        \"apiKey\":   NEWSAPI_KEY,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"NewsAPI error: {response.status_code} – {response.text}\")\n",
    "        return []\n",
    "\n",
    "    raw = response.json().get(\"articles\", [])\n",
    "    print(f\"→ NewsAPI: {len(raw)} articles fetched.\")\n",
    "\n",
    "    # build our uniform article dicts\n",
    "    all_articles = [\n",
    "        {\n",
    "            \"source\":            f\"{a['source']['name']} [NewsAPI]\",\n",
    "            \"url\":               a[\"url\"],\n",
    "            \"title\":             a[\"title\"],\n",
    "            \"published_at\":      a[\"publishedAt\"],\n",
    "            \"content\":           a.get(\"content\") or a.get(\"description\", \"\"),\n",
    "            \"platforms_mentioned\": [],\n",
    "        }\n",
    "        for a in raw\n",
    "    ]\n",
    "\n",
    "    # apply QUERY-based filtering if ENABLE_FILTERING is True\n",
    "    return apply_query_filter(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c1cd47-ca83-492f-89c8-33c99dfb88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SEC FETCH ==========\n",
    "def fetch_sec_press_releases():\n",
    "    RSS_URL = \"https://www.sec.gov/news/pressreleases.rss\"\n",
    "    feed = feedparser.parse(RSS_URL)\n",
    "\n",
    "    entries = []\n",
    "    for e in feed.entries:\n",
    "        entries.append({\n",
    "            \"source\":            \"SEC Press Releases [RSS]\",\n",
    "            \"url\":               e.link,\n",
    "            \"title\":             e.title,\n",
    "            \"published_at\":      getattr(e, \"published\", \"\"),\n",
    "            \"content\":           e.get(\"summary\", \"\"),\n",
    "            \"platforms_mentioned\": [],\n",
    "        })\n",
    "\n",
    "    print(f\"→ SEC Press Releases: {len(entries)} fetched.\")\n",
    "    # apply QUERY-based filtering if ENABLE_FILTERING is True\n",
    "    return apply_query_filter(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58aa8b87-8942-40d5-8d85-bdf39ebf7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== GNEWS FETCH ==========\n",
    "def fetch_gnews_financial_times():\n",
    "    # show the actual short query you’re using\n",
    "    print(f\"Fetching from GNews (query: '{QUERY_short}')…\")\n",
    "    \n",
    "    api_key = \"c4f8fe7bbdaea71cd2ec22279906c40f\"\n",
    "    url     = \"https://gnews.io/api/v4/search\"\n",
    "    params  = {\n",
    "        \"q\":       QUERY_short,\n",
    "        \"in\":      \"title,description\",\n",
    "        \"lang\":    LANGUAGE,\n",
    "        \"country\": \"us\",\n",
    "        \"max\":     PAGE_SIZE,\n",
    "        \"token\":   api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"GNews error: {response.status_code} – {response.text}\")\n",
    "        return []\n",
    "\n",
    "    raw = response.json().get(\"articles\", [])\n",
    "    print(f\"→ GNews: {len(raw)} articles fetched'.\")\n",
    "\n",
    "    all_articles = []\n",
    "    for a in raw:\n",
    "        source_name = a.get(\"source\", {}).get(\"name\", \"N/A\")\n",
    "        all_articles.append({\n",
    "            \"source\":            f\"{source_name} [GNews]\",\n",
    "            \"url\":               a.get(\"url\", \"\"),\n",
    "            \"title\":             a.get(\"title\", \"\"),\n",
    "            \"published_at\":      a.get(\"publishedAt\", \"\"),\n",
    "            \"content\":           a.get(\"description\", \"\"),\n",
    "            \"platforms_mentioned\": [],\n",
    "        })\n",
    "\n",
    "    return apply_query_filter(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c353e96-0701-438e-ac37-b4549d05740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== INVESTING.COM RSS FETCH ==========\n",
    "def fetch_investing_rss():\n",
    "    print(\"Fetching Investing.com RSS feeds...\")\n",
    "    feeds = {\n",
    "        \"Investing.com (English) [RSS]\": \"https://www.investing.com/rss/news_25.rss?limit=20\",\n",
    "        \"Investing.com (German)  [RSS]\": \"https://de.investing.com/rss/news_95.rss\"\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    for label, feed_url in feeds.items():\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        for entry in feed.entries:\n",
    "            articles.append({\n",
    "                \"source\":             label,\n",
    "                \"url\":                entry.link,\n",
    "                \"title\":              entry.title,\n",
    "                \"published_at\":       entry.published if \"published\" in entry else \"\",\n",
    "                \"content\":            entry.get(\"summary\", \"\"),\n",
    "                \"platforms_mentioned\": [],\n",
    "            })\n",
    "\n",
    "    print(f\"→ Investing.com RSS: {len(articles)} articles fetched.\")\n",
    "\n",
    "    # apply QUERY-based filtering if ENABLE_FILTERING is True\n",
    "    return apply_query_filter(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00834093-1908-42c8-ba5c-717f733ef848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CRUNCHBASE FETCH ==========\n",
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "\n",
    "def fetch_crunchbase_sections():\n",
    "    \"\"\"\n",
    "    Scrape three Crunchbase News sections and deep‑fetch each\n",
    "    article’s JSON‑LD to extract a proper published_at and content.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://news.crunchbase.com\"\n",
    "    sections = [\n",
    "        {\n",
    "            \"label\": \"Crunchbase News – Fintech [Scrape]\",\n",
    "            \"url\": f\"{BASE_URL}/sections/fintech-ecommerce/\",\n",
    "            \"keywords\": {\"lending\", \"credit\", \"finance\", \"regulation\", \"regulations\"},\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Crunchbase News – IPO [Scrape]\",\n",
    "            \"url\": f\"{BASE_URL}/sections/public/ipo/\",\n",
    "            \"keywords\": None,\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Crunchbase News – Seed Funding [Scrape]\",\n",
    "            \"url\": f\"{BASE_URL}/sections/seed/\",\n",
    "            \"keywords\": None,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    articles = []\n",
    "\n",
    "    for sec in sections:\n",
    "        section_resp = requests.get(sec[\"url\"], headers=headers)\n",
    "        section_resp.raise_for_status()\n",
    "        soup = BeautifulSoup(section_resp.text, \"lxml\")\n",
    "\n",
    "        # each H2 with a link is one article teaser on the section page\n",
    "        for h2 in soup.find_all(\"h2\"):\n",
    "            link_tag = h2.find(\"a\", href=True)\n",
    "            if not link_tag:\n",
    "                continue\n",
    "\n",
    "            title = link_tag.get_text(strip=True)\n",
    "            href  = link_tag[\"href\"]\n",
    "            url   = href if href.startswith(\"http\") else (BASE_URL + href)\n",
    "\n",
    "            # now deep‑fetch the article page\n",
    "            art = requests.get(url, headers=headers)\n",
    "            art.raise_for_status()\n",
    "            art_soup = BeautifulSoup(art.text, \"lxml\")\n",
    "\n",
    "            # find the JSON‑LD with \"@type\": \"NewsArticle\"\n",
    "            published_iso = \"\"\n",
    "            content_snip = \"\"\n",
    "            for script in art_soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "                try:\n",
    "                    data = json.loads(script.string)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # handle list or single object\n",
    "                if isinstance(data, list):\n",
    "                    # find the NewsArticle entry\n",
    "                    for entry in data:\n",
    "                        if entry.get(\"@type\") == \"NewsArticle\":\n",
    "                            data = entry\n",
    "                            break\n",
    "                if data.get(\"@type\") != \"NewsArticle\":\n",
    "                    continue\n",
    "\n",
    "                # extract publish date\n",
    "                dp = data.get(\"datePublished\") or data.get(\"uploadDate\")\n",
    "                if dp:\n",
    "                    try:\n",
    "                        # normalize to ISO 8601 UTC\n",
    "                        dt = parser.isoparse(dp)\n",
    "                        published_iso = dt.date().isoformat() \n",
    "                    except Exception:\n",
    "                        ppublished_iso = dp.split(\"T\")[0] if \"T\" in dp else dp\n",
    "                # extract a snippet: articleBody is full text, description is summary\n",
    "                content_snip = data.get(\"description\") or data.get(\"articleBody\",\"\")\n",
    "                break  # stop after first NewsArticle\n",
    "\n",
    "            # if JSON-LD failed, you could fallback to section‑page teaser\n",
    "            if not content_snip:\n",
    "                p = h2.find_next_sibling(\"p\")\n",
    "                content_snip = p.get_text(strip=True) if p else \"\"\n",
    "\n",
    "            # apply your keyword filter only on Fintech section\n",
    "            if sec[\"keywords\"]:\n",
    "                txt = (title + \" \" + content_snip).lower()\n",
    "                if not any(k in txt for k in sec[\"keywords\"]):\n",
    "                    continue\n",
    "\n",
    "            articles.append({\n",
    "                \"source\":    sec[\"label\"],\n",
    "                \"url\":       url,\n",
    "                \"title\":     title,\n",
    "                \"published_at\": published_iso,\n",
    "                \"content\":     content_snip,\n",
    "                \"platforms_mentioned\": [],\n",
    "            })\n",
    "\n",
    "    print(f\"→ Crunchbase News (all sections): {len(articles)} fetched.\")\n",
    "    # apply QUERY-based filtering if ENABLE_FILTERING is True\n",
    "    return apply_query_filter(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3040704-1da6-451e-bf33-d67fbfd7fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNBC_RSS_FEEDS = {\n",
    "    \"CNBC Top News\":      \"https://www.cnbc.com/id/100003114/device/rss/rss.html\",\n",
    "    \"CNBC Markets\":       \"https://www.cnbc.com/id/19746125/device/rss/rss.html\",\n",
    "    \"CNBC Technology\":    \"https://www.cnbc.com/id/10000115/device/rss/rss.html\",\n",
    "    \"CNBC Finance\":       \"https://www.cnbc.com/id/10000664/device/rss/rss.html\",\n",
    "    \"CNBC Personal Fin.\": \"https://www.cnbc.com/id/21324812/device/rss/rss.html\",\n",
    "}\n",
    "\n",
    "def fetch_cnbc_rss():\n",
    "    print(\"Fetching CNBC RSS feeds…\")\n",
    "    articles = []\n",
    "\n",
    "    for label, url in CNBC_RSS_FEEDS.items():\n",
    "        feed = feedparser.parse(url)\n",
    "        if getattr(feed, \"bozo\", False):\n",
    "            print(f\"  ⚠️  Failed to parse {label}: {feed.bozo_exception}\")\n",
    "            continue\n",
    "\n",
    "        for entry in feed.entries:\n",
    "            # fallback logic for summary/description/published date\n",
    "            summary = getattr(entry, \"summary\", \"\")\n",
    "            if not summary:\n",
    "                summary = entry.get(\"description\", \"\")\n",
    "            published = getattr(entry, \"published\", entry.get(\"pubDate\", \"\"))\n",
    "\n",
    "            articles.append({\n",
    "                \"source\":       f\"{label} [RSS]\",\n",
    "                \"url\":          entry.get(\"link\", \"\"),\n",
    "                \"title\":        entry.get(\"title\", \"\").strip(),\n",
    "                \"published_at\": published,\n",
    "                \"content\":      summary.strip(),\n",
    "                \"platforms_mentioned\": [],\n",
    "            })\n",
    "\n",
    "    print(f\"→ CNBC RSS: {len(articles)} articles fetched.\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc060800-dd48-4bb8-bd14-7dbb494a698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAHOO_FINANCE_RSS_FEEDS = {\n",
    "    \"Top Stories\":     \"https://finance.yahoo.com/rss/topstories\",\n",
    "    \"News Index\":      \"https://finance.yahoo.com/news/rssindex\",\n",
    "    \"All Finance\":     \"https://finance.yahoo.com/news/rss\",\n",
    "    # …add more (e.g. symbol-specific via \n",
    "    #    f\"http://finance.yahoo.com/rss/headline?s={symbol}\"\n",
    "    # ) if you need ticker-level feeds\n",
    "}\n",
    "\n",
    "def fetch_yahoo_rss():\n",
    "    print(\"Fetching Yahoo Finance websites…\")\n",
    "    articles = []\n",
    "\n",
    "    for label, url in YAHOO_FINANCE_RSS_FEEDS.items():\n",
    "        feed = feedparser.parse(url)\n",
    "        if getattr(feed, \"bozo\", False):\n",
    "            print(f\"  ⚠️  Failed to parse '{label}': {feed.bozo_exception}\")\n",
    "            continue\n",
    "\n",
    "        for entry in feed.entries:\n",
    "            # summary/description fallback\n",
    "            summary = getattr(entry, \"summary\", \"\") or entry.get(\"description\", \"\")\n",
    "            # published date fallback\n",
    "            published = getattr(entry, \"published\", \"\") or entry.get(\"pubDate\", \"\")\n",
    "\n",
    "            articles.append({\n",
    "                \"source\":       f\"{label} [RSS]\",\n",
    "                \"url\":          entry.get(\"link\", \"\"),\n",
    "                \"title\":        entry.get(\"title\", \"\").strip(),\n",
    "                \"published_at\": published,\n",
    "                \"content\":      summary.strip(),\n",
    "                \"platforms_mentioned\": [],\n",
    "            })\n",
    "\n",
    "    print(f\"→ Yahoo Finance: {len(articles)} articles fetched.\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "145a4759-7c34-4d2a-8892-e605cdcf09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Sifted FETCH ==========\n",
    "def fetch_sifted_rss():\n",
    "    print(\"Fetching Sifted RSS feeds…\")\n",
    "    feeds = {\"Sifted\": \"https://sifted.eu/feed/\"}\n",
    "    articles = []\n",
    "\n",
    "    for label, feed_url in feeds.items():\n",
    "        resp = requests.get(feed_url, timeout=10, headers={\"User-Agent\": \"MyBot/1.0\"})\n",
    "        feed = feedparser.parse(resp.content)\n",
    "\n",
    "        for entry in feed.entries:\n",
    "            content = getattr(entry, \"summary\", entry.get(\"description\", \"\"))\n",
    "            articles.append({\n",
    "                \"source\":            f\"Sifted — {label} [RSS]\",\n",
    "                \"url\":               entry.link,\n",
    "                \"title\":             entry.title,\n",
    "                \"published_at\":      entry.get(\"published\", \"\"),\n",
    "                \"content\":           content,\n",
    "                \"platforms_mentioned\": [],\n",
    "            })\n",
    "\n",
    "    print(f\"→ Sifted RSS: {len(articles)} articles fetched.\")\n",
    "    return apply_query_filter(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41cb3b0-4abf-4937-9981-1169d90db6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SAVE ==========\n",
    "# ── Compute a repo-relative data directory ──────────────────────────────────────\n",
    "# In Actions, cwd() will be /github/workspace; locally it'll be wherever you launch Jupyter.\n",
    "BASE_DIR = Path().cwd()\n",
    "SAVE_DIR = BASE_DIR / \"data\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def save_articles(articles):\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    filepath = SAVE_DIR / f\"news_{today}.json\"\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, indent=2)\n",
    "    print(f\"✅ Saved {len(articles)} articles to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b93fa57d-9f11-42ed-a0ac-124d74edcd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from NewsAPI...\n",
      "→ NewsAPI: 98 articles fetched.\n",
      "Fetching Bloomberg RSS feeds...\n",
      "→ Bloomberg RSS: 136 articles fetched.\n",
      "Fetching from GNews (query: 'credit OR loan OR Exaloan OR lending')…\n",
      "→ GNews: 10 articles fetched'.\n",
      "Fetching Investing.com RSS feeds...\n",
      "→ Investing.com RSS: 20 articles fetched.\n",
      "→ SEC Press Releases: 25 fetched.\n",
      "→ Crunchbase News (all sections): 24 fetched.\n",
      "Fetching CNBC RSS feeds…\n",
      "→ CNBC RSS: 150 articles fetched.\n",
      "Fetching Yahoo Finance RSS feeds…\n",
      "→ Yahoo Finance RSS: 141 articles fetched.\n",
      "Fetching Sifted RSS feeds…\n",
      "→ Sifted RSS: 24 articles fetched.\n",
      "✅ Saved 628 articles to /Users/florianterne/Downloads/data/news_2025-05-25.json\n"
     ]
    }
   ],
   "source": [
    "# ========== RUN ==========\n",
    "newsapi_articles     = fetch_newsapi()\n",
    "rss_articles         = fetch_bloomberg_rss()\n",
    "gnews_articles       = fetch_gnews_financial_times()\n",
    "investing_articles   = fetch_investing_rss()\n",
    "sec_articles         = fetch_sec_press_releases()\n",
    "crunchbase_articles  = fetch_crunchbase_sections()\n",
    "cnbc_articles        = fetch_cnbc_rss()\n",
    "yahoo_articles       = fetch_yahoo_rss()\n",
    "sifted_articles      = fetch_sifted_rss()\n",
    "\n",
    "all_articles = (\n",
    "    rss_articles\n",
    "  + newsapi_articles\n",
    "  + gnews_articles\n",
    "  + investing_articles\n",
    "  + sec_articles\n",
    "  + crunchbase_articles\n",
    "  + cnbc_articles\n",
    "  + yahoo_articles\n",
    "  + sifted_articles\n",
    ")\n",
    "\n",
    "# Add keywords to each article\n",
    "for article in all_articles:\n",
    "    full_text = f\"{article.get('title', '')} {article.get('content', '')}\"\n",
    "    article[\"keywords\"] = extract_keywords(full_text)\n",
    "\n",
    "# Save to daily file with keywords included\n",
    "if all_articles:\n",
    "    save_articles(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a4059-a4f3-4b67-bc8c-ca812da99b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
